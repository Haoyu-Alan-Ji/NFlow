{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e8de25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z: tensor([-0.3040,  0.2222,  0.8658,  0.2976, -1.6826])\n",
      "x: tensor([1.7872, 2.1556, 2.6060, 2.2083, 0.8222])\n",
      "x_hat: tensor([1.7872, 2.1556, 2.6060, 2.2083, 0.8222])\n",
      "abs error: tensor([0., 0., 0., 0., 0.])\n",
      "max abs error: 0.0\n",
      "log p(x): tensor([-0.6085, -0.5870, -0.9370, -0.6065, -1.9778])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor(0.7) \n",
    "b = torch.tensor(2.0)\n",
    "\n",
    "base = torch.distributions.Normal(0.0, 1.0)\n",
    "\n",
    "def forward(z):\n",
    "    # x = f(z)\n",
    "    return a * z + b\n",
    "\n",
    "def inverse(x):\n",
    "    # z = f^{-1}(x)\n",
    "    return (x - b) / a\n",
    "\n",
    "def log_prob_x(x):\n",
    "    z = inverse(x)\n",
    "    return base.log_prob(z) - torch.log(torch.abs(a))\n",
    "\n",
    "def reconstruct_x(x):\n",
    "    # x -> z -> x_hat\n",
    "    z = inverse(x)\n",
    "    x_hat = forward(z)\n",
    "    return x_hat\n",
    "\n",
    "z = base.sample((5,))\n",
    "x = forward(z)\n",
    "\n",
    "x_hat = reconstruct_x(x)\n",
    "\n",
    "err = (x_hat - x).abs()\n",
    "\n",
    "print(\"z:\", z)\n",
    "print(\"x:\", x)\n",
    "print(\"x_hat:\", x_hat)\n",
    "print(\"abs error:\", err)\n",
    "print(\"max abs error:\", err.max().item())\n",
    "\n",
    "print(\"log p(x):\", log_prob_x(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75555a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  100 | NLL 1.0543 | a=0.7008 b=1.9704 | est_mu=1.9704 est_sigma=0.7008\n",
      "step  200 | NLL 1.0536 | a=0.6940 b=1.9948 | est_mu=1.9948 est_sigma=0.6940\n",
      "step  300 | NLL 1.0536 | a=0.6940 b=1.9947 | est_mu=1.9947 est_sigma=0.6940\n",
      "step  400 | NLL 1.0536 | a=0.6940 b=1.9947 | est_mu=1.9947 est_sigma=0.6940\n",
      "step  500 | NLL 1.0536 | a=0.6940 b=1.9947 | est_mu=1.9947 est_sigma=0.6940\n",
      "\n",
      "Target:   mu=2.000 sigma=0.700\n",
      "Learned:  mu=1.994 sigma=0.695\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ----------------------------\n",
    "# 1D Affine Flow: x = a z + b, with a = exp(s) > 0\n",
    "# ----------------------------\n",
    "class AffineFlow1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.s = nn.Parameter(torch.tensor(0.0))  # log a\n",
    "        self.b = nn.Parameter(torch.tensor(0.0))  # shift\n",
    "\n",
    "        self.base = torch.distributions.Normal(loc=0.0, scale=1.0)\n",
    "\n",
    "    @property\n",
    "    def a(self):\n",
    "        return torch.exp(self.s)\n",
    "\n",
    "    def sample(self, n: int, device=None):\n",
    "        if device is None:\n",
    "            device = self.s.device\n",
    "        z = self.base.sample((n,)).to(device)\n",
    "        x = self.a * z + self.b\n",
    "        return x\n",
    "\n",
    "    def log_prob(self, x: torch.Tensor):\n",
    "        # inverse: z = (x - b) / a\n",
    "        z = (x - self.b) / self.a\n",
    "        # log p(x) = log p(z) - log|a|\n",
    "        return self.base.log_prob(z) - self.s  # 1D, so no sum needed\n",
    "\n",
    "def main():\n",
    "    torch.manual_seed(0)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Target data (what we want the flow to fit)\n",
    "    # ----------------------------\n",
    "    true_mu = 2.0\n",
    "    true_sigma = 0.7\n",
    "    target = torch.distributions.Normal(true_mu, true_sigma)\n",
    "\n",
    "    n_data = 5000\n",
    "    x_data = target.sample((n_data,)).to(device)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Flow model\n",
    "    # ----------------------------\n",
    "    flow = AffineFlow1D().to(device)\n",
    "    opt = torch.optim.Adam(flow.parameters(), lr=5e-2)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Train by maximum likelihood: maximize log_prob(data)\n",
    "    # ----------------------------\n",
    "    for step in range(1, 501):\n",
    "        opt.zero_grad()\n",
    "        nll = -flow.log_prob(x_data).mean()  # negative log-likelihood\n",
    "        nll.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            with torch.no_grad():\n",
    "                est_mu = flow.b.item()\n",
    "                est_sigma = flow.a.item()  # since z~N(0,1), x~N(b, a^2)\n",
    "            print(f\"step {step:4d} | NLL {nll.item():.4f} | a={flow.a.item():.4f} b={flow.b.item():.4f} \"\n",
    "                  f\"| est_mu={est_mu:.4f} est_sigma={est_sigma:.4f}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Quick sanity check: sample and compare moments\n",
    "    # ----------------------------\n",
    "    with torch.no_grad():\n",
    "        xs = flow.sample(200000, device=device)\n",
    "        print(\"\\nTarget:   mu=%.3f sigma=%.3f\" % (true_mu, true_sigma))\n",
    "        print(\"Learned:  mu=%.3f sigma=%.3f\" % (xs.mean().item(), xs.std(unbiased=False).item()))\n",
    "        print(\"DONE\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
